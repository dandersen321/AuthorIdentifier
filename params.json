{"name":"Author Identifier","tagline":"Apply Artificial Intelligence to Stylometry to determine authorship of documents whose author is unknown or in question.","body":"## Executive Summary\r\n\r\nIt can be hard to tell who the true author of a document it.  This problem may arise when checking for plagiarism or old documents whose authorship is unknown.  To solve this, this tool uses a decision tree to learn the style of many authors.  Then using this information it can analyze a document and determine the original author of the work in question.\r\n\r\n## Description\r\n\r\nIt is often difficult for a human to be able to analyze a document and determine the author.  This linguistic practice is known as Stylometry and has many useful applications.  There are many ways this problem has been approached, but each typically consists of looking for similar patterns between an author and the document being analyzed.  This tool attempts to improve the accuracy of such a method by using a decision tree that can find complex patterns that might be hard for a human to discover.\r\n\r\nThis implementation was used with a dataset of 24,428 books from the Gutenberg project (https://www.gutenberg.org/) but future releases may incorporate other media such as journals and emails.  The tool will analysis each book to try and discover an author's style.  If successful, it can then use this information to predict the author of future books it has not seen before.  This tool has been written in python using nltk and sklearn packages for parsing and AI learning respectively.\r\n\r\n## Why\r\n\r\nThere are several uses that one can find for the author identifier, but I will list the main ones I see: \r\n\r\n1. Plagiarism: the tool could determine the probability that a paper under one author is actually written from another source\r\n2. Validity of works:  the tool could determine the legitimacy of works whose authorship is in question.\r\n3. Discovering authors hidden under pen names: this tool could be used to help determine the author of books that have been written under a pen name\r\n4. Similar writing styles: the tool could help determine how close different authorâ€™s writing styles are\r\n\r\n## The Process\r\n\r\n### The Analysis Phase\r\n\r\nFrom my initial research, I used 31 features in my analysis phase.  They are:\r\n* apostrophesPerWord: how many apostrophes are used in each word\r\n* averageParagraphLength: the average number of sentences in each paragraph\r\n* averageWordLength: the average character length of each word\r\n* averageWordsPerSentence: the average words in each sentence\r\n* bigraphs: how often the two character sequence appears in a word\r\n    + bigraph-co\r\n    + bigraph-lc\r\n    + bigraph-me\r\n    + bigraph-we\r\n* digitFraction: how many characters are digits\r\n* numberOfWords: how many words the document contains\r\n* uppercaseFraction: how many words start with an uppercase character\r\n* whitespaceFraction: the ratio of the whitespace to words \r\n* WPT: (Words Per Thousand) how often a word occurs out of every thousand words\r\n    + WPT-and\r\n    + WPT-because\r\n    + WPT-but\r\n    + WPT-colon\r\n    + WPT-comma\r\n    + WPT-doubleHyphen\r\n    + WPT-exclamationMark\r\n    + WPT-however\r\n    + WPT-hyphen\r\n    + WPT-if\r\n    + WPT-might\r\n    + WPT-more\r\n    + WPT-must\r\n    + WPT-quote\r\n    + WPT-semicolon\r\n    + WPT-since\r\n    + WPT-that\r\n    + WPT-this\r\n    + WPT-very\r\n\r\nI calculated these feature on each document then saved the results in a json file.  In this way we don't have to analyze each document every time the program is run (a full analysis on all 24,428 books takes about 8 hours).\r\n\r\n### Creating the decision tree\r\n\r\nTo analyze the data I used a decision tree using python library sklearn's implementation.  The decision tree works to try and compartmentalize the data into leaf nodes where each leaf contains only (or a large majority) of one author.  A simple example would be given we had 3 authors: star, circle, and triangle and we were using 2 parameters: averageParagraphLength and WPT-and.  We might end up with the following graph:\r\n\r\n![](https://raw.githubusercontent.com/dandersen321/AuthorIdentifier/master/decisionTreeBasicGraph.PNG)\r\n\r\nwhere each shape represents one book by that author.  We would then create the following decision tree:\r\n\r\n![](https://raw.githubusercontent.com/dandersen321/AuthorIdentifier/master/decisionTreeBasicTree.PNG)\r\n\r\nIn this way, if were given any data point from a book we would be able to identify the author.  Now the decision tree used for this project is quite more complicated and the data not as perfectly categorized.  If you want to get a look at the complexity of the tree you can get a look here: [decision tree pdf](https://raw.githubusercontent.com/dandersen321/AuthorIdentifier/master/decisionTree.pdf).\r\n\r\n## Results\r\n\r\nUsing this method I was able to predict the correct author with a relatively high degree of accuracy.  This accuracy was correlated with two factors: the number of authors used and the minimum number of books each author must have written to be included in the dataset.  The following graphs show the results.\r\n\r\n![](https://raw.githubusercontent.com/dandersen321/AuthorIdentifier/master/authorsUsedOnAccuracy.PNG)\r\n\r\nWe can see that as the number of authors increase, the accuracy decreases until it steadies at around 45% accuracy.  So given a high school classroom size of about 30 we would expect a 75% accuracy in the prediction of anonymous documents and given a large college course of 250 we would expect 45% accuracy.  \r\n\r\nA second but lesser impactful variable was the number of books an author needed to write to be included in the dataset.  As we can see from the following graph, the more books we have to analyze from an author results in a small positive correlation to our accuracy.\r\n\r\n![](https://raw.githubusercontent.com/dandersen321/AuthorIdentifier/master/minBooks.png) \r\n\r\n## Features of Importance\r\n\r\nAfter looking at the results, I also explored which features improved the accuracy the most.  The first way was to use only that feature and see how well I could predict the accuracy.  This resulted in expectedly low accuracy, but showing which features were strong on their own.\r\n\r\n![](https://raw.githubusercontent.com/dandersen321/AuthorIdentifier/master/SingleParam.png)\r\n\r\nI then looked at how the features combined to affect accuracy.  I did this by starting with zero features, and then added one random feature and computed it's accuracy in predicting authors.  This resulted in the following data.\r\n\r\n![](https://raw.githubusercontent.com/dandersen321/AuthorIdentifier/master/addingFeatures.PNG)\r\n\r\nAs we can see, the first features are the most signficant but after several features each additional one has a smaller impact.  We also see that certain features that did not well on the single param test also added little or even hurt the accuracy.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}